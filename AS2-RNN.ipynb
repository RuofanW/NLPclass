{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a74d7adf5c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tmp = open(\"hw2_data/snli_train.tsv\").read().split('\\n')\n",
    "train_data = [row.split('\\t') for row in train_tmp][1:-1]\n",
    "val_tmp = open(\"hw2_data/snli_val.tsv\").read().split('\\n')\n",
    "val_data = [row.split('\\t') for row in val_tmp][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_s1 = [row[0] for row in train_data]\n",
    "train_s2 = [row[1] for row in train_data]\n",
    "val_s1 = [row[0] for row in val_data]\n",
    "val_s2 = [row[1] for row in val_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = [row[2] for row in train_data]\n",
    "val_label = [row[2] for row in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "#         print(type(sample))\n",
    "        tokens = nltk.word_tokenize(sample.lower())\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_s1_tokens, _ = tokenize_dataset(val_s1)\n",
    "pkl.dump(val_s1_tokens, open(\"val_s1_tokens.p\", \"wb\"))\n",
    "val_s2_tokens, _ = tokenize_dataset(val_s2)\n",
    "pkl.dump(val_s2_tokens, open(\"val_s2_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_s1_tokens, all_train_s1_tokens = tokenize_dataset(train_s1)\n",
    "train_s2_tokens, all_train_s2_tokens = tokenize_dataset(train_s2)\n",
    "\n",
    "pkl.dump(train_s1_tokens, open(\"train_s1_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_s2_tokens, open(\"train_s2_tokens.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_s1_tokens.extend(all_train_s2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 500000\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((2+words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2words_ft[0] = '<pad>'\n",
    "idx2words_ft[1] = '<unk>'\n",
    "words_ft['<pad>'] = 0\n",
    "words_ft['<unk>'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 274270 ; token ridley\n",
      "Token ridley; token id 274270\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(idx2words_ft)-1)\n",
    "random_token = idx2words_ft[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, idx2words_ft[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, words_ft[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words_ft[token] if token in words_ft else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_s1_indices = token2index_dataset(train_s1_tokens)\n",
    "val_s1_indices = token2index_dataset(val_s1_tokens)\n",
    "train_s2_indices = token2index_dataset(train_s2_tokens)\n",
    "val_s2_indices = token2index_dataset(val_s2_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_s1_indices)))\n",
    "print (\"Train dataset size is {}\".format(len(train_s2_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s1_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s2_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 25\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s1_data, s2_data, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.s1_data = s1_data\n",
    "        self.s2_data = s2_data\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.s1_data) == len(self.target_list))\n",
    "        assert (len(self.s2_data) == len(self.target_list))\n",
    "#         self.words_ft = words_ft\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        item = dict()\n",
    "        \n",
    "        item['s1_word_idx'] = self.s1_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['s2_word_idx'] = self.s2_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['label'] = self.target_list[key]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list, s2_list = [],[]\n",
    "    label_list = []\n",
    "#     length_list = []\n",
    "#     print(batch)\n",
    "    for datum in batch:\n",
    "        label_list.append(datum['label'])\n",
    "#         length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s1_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s1_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1_list.append(list(padded_vec))\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s2_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s2_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s2_list.append(list(padded_vec))\n",
    "#     ind_dec_order = np.argsort(length_list)[::-1]\n",
    "#     data_list = np.array(data_list)[ind_dec_order]\n",
    "#     length_list = np.array(length_list)[ind_dec_order]\n",
    "#     label_list = np.array(label_list)[ind_dec_order]\n",
    "    label_list = [{'entailment':0,'contradiction':1,'neutral':2}[k] for k in label_list]\n",
    "#     print(label_list)\n",
    "    return [torch.from_numpy(np.array(s1_list)),torch.from_numpy(np.array(s2_list)), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(train_s1_indices, train_s2_indices, train_label),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(val_s1_indices, val_s2_indices, val_label),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-927f436df3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# RNN Accepts the following hyperparams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# emb_size: Embedding Size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# hidden_size: Hidden Size of layer in RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        num_ebd, emb_size = weights_matrix.shape\n",
    "        self.rnn1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(hidden_size*2*2,1000),\n",
    "            #nn.Linear(rnn_output_dim, output_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1000,num_classes),\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # reset hidden state\n",
    "\n",
    "#         batch_size = x.size()\n",
    "        batch_size = BATCH_SIZE\n",
    "#         print(len(s1))\n",
    "\n",
    "        self.hidden1 = self.init_hidden(batch_size)\n",
    "        self.hidden2 = self.init_hidden(batch_size)\n",
    "\n",
    "        # get embedding of characters\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        # pack padded sequence\n",
    "#         s1_embed = torch.nn.utils.rnn.pack_padded_sequence(s1_embed, lengths.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        s1_rnn_out, self.hidden1 = self.rnn1(s1_embed, self.hidden1)\n",
    "        s2_rnn_out, self.hidden2 = self.rnn2(s2_embed, self.hidden2)\n",
    "        # undo packing\n",
    "#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        s1_rnn_out = torch.sum(s1_rnn_out, dim=1)\n",
    "        s2_rnn_out = torch.sum(s2_rnn_out, dim=1)\n",
    "        \n",
    "        rnn_out = torch.cat([s1_rnn_out, s2_rnn_out], 1)\n",
    "        \n",
    "        logits = self.mlp(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, train=False):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    nbatch = 0\n",
    "    sumloss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    # get a random sample\n",
    "    if train:\n",
    "        sp_loader = torch.utils.data.DataLoader(\n",
    "            SNLIDataset(train_s1_indices, train_s2_indices, train_label),batch_size=BATCH_SIZE,collate_fn=vocab_collate_func,\n",
    "            sampler=SubsetRandomSampler(range(10*BATCH_SIZE)))\n",
    "    else:\n",
    "        sp_loader = torch.utils.data.DataLoader(\n",
    "            SNLIDataset(val_s1_indices, val_s2_indices, val_label),batch_size=BATCH_SIZE,collate_fn=vocab_collate_func,\n",
    "            sampler=SubsetRandomSampler(range(10*BATCH_SIZE)))\n",
    "\n",
    "    for sample in sp_loader:\n",
    "        try:\n",
    "            outputs = F.softmax(model(sample[0], sample[1]), dim=1)\n",
    "            loss = criterion(outputs, sample[2])\n",
    "            nbatch += 1\n",
    "            sumloss += loss.item()\n",
    "            predicted = outputs.max(1, keepdim=True)[1].view(-1)\n",
    "            total += len(predicted)\n",
    "            truths = sample[2]\n",
    "            correct += predicted.eq(truths.view_as(predicted)).sum().item()\n",
    "        except:\n",
    "            total += 0\n",
    "            correct += 0\n",
    "    return (100 * correct / total), sumloss/nbatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangruofan/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1363707780838013\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 36.99596774193548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ca335eed2bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=500, num_layers=1, num_classes=3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "#         print(len(sample['label']))\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(sample[0], sample[1])\n",
    "        label = sample[2]\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print(loss.item())\n",
    "#             validate\n",
    "            val_acc, val_los = test_model(model, train=False)\n",
    "            tra_acc, tra_los = test_model(model, train=True)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, num_epochs):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    t_acc, v_acc = [], []\n",
    "    t_los, v_los = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sample in enumerate(train_loader):\n",
    "    #         print(len(sample['label']))\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(sample[0], sample[1])\n",
    "            label = sample[2]\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 iterations\n",
    "            if i > 0 and i % 200 == 0:\n",
    "    #             validate\n",
    "                val_acc, val_los = test_model(model, train=False)\n",
    "                tra_acc, tra_los = test_model(model, train=True)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "                t_acc.append(tra_acc)\n",
    "                v_acc.append(val_acc)\n",
    "                t_los.append(tra_los)\n",
    "                v_los.append(val_los)\n",
    "    return t_acc, v_acc, t_los, v_los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hidden dimension of GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangruofan/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-cc05eecdbc87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaded_embeddings_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_los\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_los\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hidden size \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-c45a495c7296>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-db2167f5813c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s1, s2)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0ms1_rnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0ms2_rnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# undo packing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb_ih\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_sizes = [250,500,750]\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize= (15,12))\n",
    "ax[0][0].set_title(\"Train Accuracy\", loc=\"center\", y=1.05)\n",
    "ax[0][1].set_title(\"Val Accuracy\", loc=\"center\", y=1.05)\n",
    "ax[1][0].set_title(\"Train Loss\", loc=\"center\", y=1.05)\n",
    "ax[1][1].set_title(\"Val Loss\", loc=\"center\", y=1.05)\n",
    "for h in hidden_sizes:\n",
    "    model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=h, num_layers=1, num_classes=3)\n",
    "    t_acc, v_acc, t_los, v_los = train_model(model, learning_rate, 5)\n",
    "    ax[0][0].plot(t_acc, label=\"hidden size \"+str(h))\n",
    "    ax[0][0].legend()\n",
    "    ax[0][1].plot(v_acc, label=\"hidden size \"+str(h))\n",
    "    ax[0][1].legend()\n",
    "    ax[1][0].plot(t_los, label=\"hidden size \"+str(h))\n",
    "    ax[1][0].legend()\n",
    "    ax[1][1].plot(v_los, label=\"hidden size \"+str(h)) \n",
    "    ax[1][1].legend()\n",
    "plt.tight_layout()    \n",
    "plt.savefig(\"hiddensize.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5623a4cc8922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaded_embeddings_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=h, num_layers=1, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
