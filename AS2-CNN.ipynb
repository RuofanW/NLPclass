{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tmp = open(\"hw2_data/snli_train.tsv\").read().split('\\n')\n",
    "train_data = [row.split('\\t') for row in train_tmp][1:-1]\n",
    "val_tmp = open(\"hw2_data/snli_val.tsv\").read().split('\\n')\n",
    "val_data = [row.split('\\t') for row in val_tmp][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_s1 = [row[0] for row in train_data]\n",
    "train_s2 = [row[1] for row in train_data]\n",
    "val_s1 = [row[0] for row in val_data]\n",
    "val_s2 = [row[1] for row in val_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = [row[2] for row in train_data]\n",
    "val_label = [row[2] for row in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "#         print(type(sample))\n",
    "        tokens = nltk.word_tokenize(sample.lower())\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_s1_tokens, _ = tokenize_dataset(val_s1)\n",
    "pkl.dump(val_s1_tokens, open(\"val_s1_tokens.p\", \"wb\"))\n",
    "val_s2_tokens, _ = tokenize_dataset(val_s2)\n",
    "pkl.dump(val_s2_tokens, open(\"val_s2_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_s1_tokens, all_train_s1_tokens = tokenize_dataset(train_s1)\n",
    "train_s2_tokens, all_train_s2_tokens = tokenize_dataset(train_s2)\n",
    "\n",
    "pkl.dump(train_s1_tokens, open(\"train_s1_tokens.p\", \"wb\"))\n",
    "pkl.dump(train_s2_tokens, open(\"train_s2_tokens.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_s1_tokens.extend(all_train_s2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((2+600000, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i  >= 600000:\n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2words_ft[0] = '<pad>'\n",
    "idx2words_ft[1] = '<unk>'\n",
    "words_ft['<pad>'] = 0\n",
    "words_ft['<unk>'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 507635 ; token scouter\n",
      "Token scouter; token id 507635\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(idx2words_ft)-1)\n",
    "random_token = idx2words_ft[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, idx2words_ft[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, words_ft[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words_ft[token] if token in words_ft else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_s1_indices = token2index_dataset(train_s1_tokens)\n",
    "val_s1_indices = token2index_dataset(val_s1_tokens)\n",
    "train_s2_indices = token2index_dataset(train_s2_tokens)\n",
    "val_s2_indices = token2index_dataset(val_s2_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_s1_indices)))\n",
    "print (\"Train dataset size is {}\".format(len(train_s2_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s1_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s2_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 40\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s1_data, s2_data, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.s1_data = s1_data\n",
    "        self.s2_data = s2_data\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.s1_data) == len(self.target_list))\n",
    "        assert (len(self.s2_data) == len(self.target_list))\n",
    "#         self.words_ft = words_ft\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        item = dict()\n",
    "        \n",
    "        item['s1_word_idx'] = self.s1_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['s2_word_idx'] = self.s2_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['label'] = self.target_list[key]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list, s2_list = [],[]\n",
    "    label_list = []\n",
    "#     length_list = []\n",
    "#     print(batch)\n",
    "    for datum in batch:\n",
    "        label_list.append(datum['label'])\n",
    "#         length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s1_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s1_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1_list.append(list(padded_vec))\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s2_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s2_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s2_list.append(list(padded_vec))\n",
    "        \n",
    "    label_list = [{'entailment':0,'contradiction':1,'neutral':2}[k] for k in label_list]\n",
    "#     print(label_list)\n",
    "    return [torch.from_numpy(np.array(s1_list)),torch.from_numpy(np.array(s2_list)), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(train_s1_indices, train_s2_indices, train_label),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(val_s1_indices, val_s2_indices, val_label),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sample in loader:\n",
    "        try:\n",
    "            outputs = F.softmax(model(sample[0], sample[1]), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1].view(-1)\n",
    "            total += len(predicted)\n",
    "            truths = sample[2]\n",
    "            correct += predicted.eq(truths.view_as(predicted)).sum().item()\n",
    "        except:\n",
    "            total += 0\n",
    "            correct += 0\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, k=3, p=1):\n",
    "        super().__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        num_ebd, emb_size = weights_matrix.shape\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=k, padding=p)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=k, padding=p)\n",
    "#         self.cnn3 = nn.Conv1d(emb_size, hidden_size, kernel_size=5, padding=2)\n",
    "#         self.cnn4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu=nn.ReLU()  \n",
    "        self.drop = nn.Dropout(p = 0.2)\n",
    "#         self.maxpool = nn.MaxPool1d(MAX_SENTENCE_LENGTH)\n",
    "#         self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.linear2 = nn.Linear(hidden_size*2, num_classes)\n",
    "        \n",
    "    def forward(self, s1, s2):\n",
    "        # reset hidden state\n",
    "\n",
    "#         batch_size = x.size()\n",
    "        batch_size1, seq_len1 = s1.size()\n",
    "        batch_size2, seq_len2 = s2.size()\n",
    "#         print(len(s1)）\n",
    "\n",
    "        # get embedding of characters\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        \n",
    "        hidden1 = self.conv1(s1_embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = self.conv1(s2_embed.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, hidden1.size(1), hidden1.size(-1))\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, hidden2.size(1), hidden2.size(-1))\n",
    "        \n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, hidden1.size(1), hidden1.size(-1))\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, hidden2.size(1), hidden2.size(-1))\n",
    "        \n",
    "        hidden1 = torch.sum(hidden1, dim=1)\n",
    "        hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        cnn_out = torch.cat([hidden1, hidden2], 1)\n",
    "        logits = self.relu(self.linear1(cnn_out))\n",
    "        logits = self.drop(logits)\n",
    "        logits = self.linear2(logits)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0648167133331299\n",
      "Epoch: [1/10], Step: [11/3125], Validation Acc: 35.3\n",
      "1.147667646408081\n",
      "Epoch: [1/10], Step: [21/3125], Validation Acc: 33.8\n",
      "1.1114165782928467\n",
      "Epoch: [1/10], Step: [31/3125], Validation Acc: 37.5\n",
      "1.062389612197876\n",
      "Epoch: [1/10], Step: [41/3125], Validation Acc: 34.5\n",
      "1.114782452583313\n",
      "Epoch: [1/10], Step: [51/3125], Validation Acc: 38.0\n",
      "1.1038511991500854\n",
      "Epoch: [1/10], Step: [61/3125], Validation Acc: 38.7\n",
      "1.1240640878677368\n",
      "Epoch: [1/10], Step: [71/3125], Validation Acc: 33.6\n",
      "1.1082526445388794\n",
      "Epoch: [1/10], Step: [81/3125], Validation Acc: 39.46280991735537\n",
      "1.1282885074615479\n",
      "Epoch: [1/10], Step: [91/3125], Validation Acc: 37.6\n",
      "1.1019346714019775\n",
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 36.0\n",
      "1.1177693605422974\n",
      "Epoch: [1/10], Step: [111/3125], Validation Acc: 38.8\n",
      "1.0576212406158447\n",
      "Epoch: [1/10], Step: [121/3125], Validation Acc: 39.4\n",
      "1.112605094909668\n",
      "Epoch: [1/10], Step: [131/3125], Validation Acc: 38.7\n",
      "1.0809595584869385\n",
      "Epoch: [1/10], Step: [141/3125], Validation Acc: 38.8\n",
      "1.0905274152755737\n",
      "Epoch: [1/10], Step: [151/3125], Validation Acc: 39.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c87549d7a15b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         print(len(sample['label']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlpclass/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f59878b0f4d2>\u001b[0m in \u001b[0;36mvocab_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                 \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's2_word_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                 mode=\"constant\", constant_values=0)\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0ms2_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'entailment'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'contradiction'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neutral'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = 2(weights_matrix=loaded_embeddings_ft, hidden_size=50, num_layers=2, num_classes=3)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "#         print(len(sample['label']))\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(sample[0], sample[1])\n",
    "        label = sample[2]\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print(loss.item())\n",
    "#             validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ma2: 2/2\n",
    "arima: 0/2\n",
    "acf: 2/2\n",
    "hmm: 2/2\n",
    "hmm&alp: 1/2\n",
    "pred: 2/3\n",
    "hmm_draw: 1/1\n",
    "arrow: 3/3\n",
    "arrow: 3/3\n",
    "A: 0/2\n",
    "alp&v: 1/2\n",
    "A^m: 1/2\n",
    "ploy: 4/4\n",
    "parti: 2/2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
