{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import nltk\n",
    "import os.path as osp\n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = open(\"hw2_data/mnli_train.tsv\",'r+', encoding=\"utf-8\").read().split('\\n')\n",
    "train_data = [row.split('\\t') for row in train_tmp][1:-1]\n",
    "val_tmp = open(\"hw2_data/mnli_val.tsv\",'r+', encoding=\"utf-8\").read().split('\\n')\n",
    "val_data = [row.split('\\t') for row in val_tmp][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 900000\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec','r+', encoding=\"utf-8\") as f:\n",
    "    loaded_embeddings_ft = np.zeros((2+words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2words_ft[0] = '<pad>'\n",
    "idx2words_ft[1] = '<unk>'\n",
    "words_ft['<pad>'] = 0\n",
    "words_ft['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s1 = [row[0] for row in train_data]\n",
    "train_s2 = [row[1] for row in train_data]\n",
    "val_s1 = [row[0] for row in val_data]\n",
    "val_s2 = [row[1] for row in val_data]\n",
    "train_label = [row[2] for row in train_data]\n",
    "val_label = [row[2] for row in val_data]\n",
    "train_genre = [row[3] for row in train_data]\n",
    "val_genre = [row[3] for row in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "#         print(type(sample))\n",
    "        tokens = nltk.word_tokenize(sample.lower())\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_s1_tokens, _ = tokenize_dataset(val_s1)\n",
    "pkl.dump(val_s1_tokens, open(\"val_s1_tokens.p\", \"wb\"))\n",
    "val_s2_tokens, _ = tokenize_dataset(val_s2)\n",
    "pkl.dump(val_s2_tokens, open(\"val_s2_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_s1_tokens, all_train_s1_tokens = tokenize_dataset(train_s1)\n",
    "train_s2_tokens, all_train_s2_tokens = tokenize_dataset(train_s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Val dataset size is 5000\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words_ft[token] if token in words_ft else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_s1_indices = token2index_dataset(train_s1_tokens)\n",
    "val_s1_indices = token2index_dataset(val_s1_tokens)\n",
    "train_s2_indices = token2index_dataset(train_s2_tokens)\n",
    "val_s2_indices = token2index_dataset(val_s2_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_s1_indices)))\n",
    "print (\"Train dataset size is {}\".format(len(train_s2_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s1_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_s2_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 30\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, s1_data, s2_data, target_list, genre_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.s1_data = s1_data\n",
    "        self.s2_data = s2_data\n",
    "        self.target_list = target_list\n",
    "        self.genre_list = genre_list\n",
    "        assert (len(self.s1_data) == len(self.target_list))\n",
    "        assert (len(self.s2_data) == len(self.target_list))\n",
    "#         self.words_ft = words_ft\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        item = dict()\n",
    "        \n",
    "        item['s1_word_idx'] = self.s1_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['s2_word_idx'] = self.s2_data[key][:MAX_SENTENCE_LENGTH]\n",
    "        item['label'] = self.target_list[key]\n",
    "        item['genre'] = self.genre_list[key]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list, s2_list = [],[]\n",
    "    label_list = []\n",
    "    genre_list = []\n",
    "#     length_list = []\n",
    "#     print(batch)\n",
    "    for datum in batch:\n",
    "        label_list.append(datum['label'])\n",
    "        genre_list.append(datum['genre'])\n",
    "#         length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s1_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s1_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1_list.append(list(padded_vec))\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum['s2_word_idx']),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-len(datum['s2_word_idx']))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s2_list.append(list(padded_vec))\n",
    "    label_list = [{'entailment':0,'contradiction':1,'neutral':2}[k] for k in label_list]\n",
    "    genre_list = [{'slate':0,'telephone':1,'government':2,'travel':3, 'fiction':4}[k] for k in genre_list]\n",
    "    return [torch.from_numpy(np.array(s1_list)),torch.from_numpy(np.array(s2_list)), torch.LongTensor(label_list),torch.LongTensor(genre_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(train_s1_indices, train_s2_indices, train_label, train_genre),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    SNLIDataset(val_s1_indices, val_s2_indices, val_label, val_genre),batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get genre dataloader\n",
    "def getgenreloader_train(genre):\n",
    "    genrels = train_genre\n",
    "    subid = [genrels[i] == genre for i in range(len(genrels))]\n",
    "    train_s1_indices_sub = [train_s1_indices[i] for i in range(len(subid)) if subid[i]]\n",
    "    train_s2_indices_sub = [train_s2_indices[i] for i in range(len(subid)) if subid[i]]\n",
    "    train_label_sub = [train_label[i] for i in range(len(subid)) if subid[i]]\n",
    "    train_genre_sub = [train_genre[i] for i in range(len(subid)) if subid[i]]\n",
    "    return torch.utils.data.DataLoader(\n",
    "        SNLIDataset(train_s1_indices_sub, train_s2_indices_sub, train_label_sub, train_genre_sub),\n",
    "        batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)\n",
    "def getgenreloader_val(genre):\n",
    "    genrels = val_genre\n",
    "    subid = [genrels[i] == genre for i in range(len(genrels))]\n",
    "    val_s1_indices_sub = [val_s1_indices[i] for i in range(len(subid)) if subid[i]]\n",
    "    val_s2_indices_sub = [val_s2_indices[i] for i in range(len(subid)) if subid[i]]\n",
    "    val_label_sub = [val_label[i] for i in range(len(subid)) if subid[i]]\n",
    "    val_genre_sub = [val_genre[i] for i in range(len(subid)) if subid[i]]\n",
    "    return torch.utils.data.DataLoader(\n",
    "        SNLIDataset(val_s1_indices_sub, val_s2_indices_sub, val_label_sub, val_genre_sub),\n",
    "        batch_size=BATCH_SIZE,shuffle=True,collate_fn=vocab_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, sp_loader, criterion=torch.nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for sample in sp_loader:\n",
    "        outputs = F.softmax(model(sample[0], sample[1]), dim=1)\n",
    "#         loss = criterion(outputs, sample[2])\n",
    "#         sumloss += loss.item()\n",
    "        predicted = outputs.max(1, keepdim=True)[1].view(-1)\n",
    "        total += len(predicted)\n",
    "        truths = sample[2]\n",
    "        correct += predicted.eq(truths.view_as(predicted)).sum().item()\n",
    "\n",
    "    return (100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slate\n",
      "telephone\n",
      "government\n",
      "travel\n",
      "fiction\n"
     ]
    }
   ],
   "source": [
    "for g in set(val_genre):\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, dropoutp=0.3):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix), freeze=True)\n",
    "        num_ebd, emb_size = weights_matrix.shape\n",
    "        self.rnn1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=dropoutp, bidirectional=True)\n",
    "        self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=dropoutp, bidirectional=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(hidden_size*2*2,400),\n",
    "            #nn.Linear(rnn_output_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropoutp),\n",
    "            nn.Linear(400,num_classes),\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # reset hidden state\n",
    "\n",
    "#         batch_size = x.size()\n",
    "        batch_size1 = s1.size()[0]\n",
    "        batch_size2 = s2.size()[0]\n",
    "    \n",
    "#         print(len(s1))\n",
    "\n",
    "        self.hidden1 = self.init_hidden(batch_size1)\n",
    "        self.hidden2 = self.init_hidden(batch_size2)\n",
    "\n",
    "        # get embedding of characters\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        # pack padded sequence\n",
    "#         s1_embed = torch.nn.utils.rnn.pack_padded_sequence(s1_embed, lengths.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        s1_rnn_out, self.hidden1 = self.rnn1(s1_embed, self.hidden1)\n",
    "        s2_rnn_out, self.hidden2 = self.rnn2(s2_embed, self.hidden2)\n",
    "        # undo packing\n",
    "#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        s1_rnn_out = torch.sum(s1_rnn_out, dim=1)\n",
    "        s2_rnn_out = torch.sum(s2_rnn_out, dim=1)\n",
    "        \n",
    "        rnn_out = torch.cat([s1_rnn_out, s2_rnn_out], 1)\n",
    "        \n",
    "        logits = self.mlp(rnn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=100, num_layers=1, num_classes=3) \n",
    "# model.load_state_dict()\n",
    "model.load_state_dict(torch.load('rnn_p_0.pth')['model'])\n",
    "# model = torch.load('rnn_p_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slate  45.60878243512974\n",
      "telephone  43.38308457711443\n",
      "government  46.653543307086615\n",
      "travel  45.926680244399186\n",
      "fiction  42.41206030150754\n"
     ]
    }
   ],
   "source": [
    "for g in set(val_genre):\n",
    "    loader = getgenreloader_val(g)\n",
    "    print(g, ' ', test_model(model, loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, k=3, p=1, dropoutp=0.3):\n",
    "        super().__init__()\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix))\n",
    "        num_ebd, emb_size = weights_matrix.shape\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=k, padding=p)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=k, padding=p)\n",
    "#         self.cnn3 = nn.Conv1d(emb_size, hidden_size, kernel_size=5, padding=2)\n",
    "#         self.cnn4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu=nn.ReLU()  \n",
    "        self.dropout = nn.Dropout(p = dropoutp)\n",
    "#         self.maxpool = nn.MaxPool1d(MAX_SENTENCE_LENGTH)\n",
    "#         self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size*2)\n",
    "        self.linear2 = nn.Linear(hidden_size*2, num_classes)\n",
    "        \n",
    "    def forward(self, s1, s2):\n",
    "        # reset hidden state\n",
    "\n",
    "#         batch_size = x.size()\n",
    "        batch_size1, seq_len1 = s1.size()\n",
    "        batch_size2, seq_len2 = s2.size()\n",
    "#         print(len(s1)）\n",
    "\n",
    "        # get embedding of characters\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        \n",
    "        hidden1 = self.conv1(s1_embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = self.conv1(s2_embed.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, hidden1.size(1), hidden1.size(-1))\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, hidden2.size(1), hidden2.size(-1))\n",
    "        \n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, hidden1.size(1), hidden1.size(-1))\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, hidden2.size(1), hidden2.size(-1))\n",
    "        \n",
    "        hidden1 = torch.sum(hidden1, dim=1)\n",
    "        hidden2 = torch.sum(hidden2, dim=1)\n",
    "        \n",
    "        cnn_out = torch.cat([hidden1, hidden2], 1)\n",
    "        logits = self.relu(self.linear1(cnn_out))\n",
    "        logits = self.dropout(logits)\n",
    "        logits = self.linear2(logits)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(weights_matrix=loaded_embeddings_ft, hidden_size=300, num_layers=1, num_classes=3, k=5) \n",
    "# model.load_state_dict()\n",
    "model.load_state_dict(torch.load('cnn_final.pth')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slate   40.31936127744511\n",
      "telephone   44.37810945273632\n",
      "government   42.32283464566929\n",
      "travel   43.788187372708755\n",
      "fiction   47.1356783919598\n"
     ]
    }
   ],
   "source": [
    "for g in set(val_genre):\n",
    "    loader = getgenreloader_val(g)\n",
    "    print(g, ' ', test_model(model, loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "slate\n",
      "Epoch: [1/3], Step: [41/126], Validation Acc: 45.13164431197218\n",
      "Epoch: [1/3], Step: [81/126], Validation Acc: 46.42324888226528\n",
      "Epoch: [1/3], Step: [121/126], Validation Acc: 47.64033780427223\n",
      "Epoch: [2/3], Step: [41/126], Validation Acc: 48.484848484848484\n",
      "Epoch: [2/3], Step: [81/126], Validation Acc: 49.72677595628415\n",
      "Epoch: [2/3], Step: [121/126], Validation Acc: 51.465474416294086\n",
      "Epoch: [3/3], Step: [41/126], Validation Acc: 51.8628912071535\n",
      "Epoch: [3/3], Step: [81/126], Validation Acc: 53.129657228017884\n",
      "Epoch: [3/3], Step: [121/126], Validation Acc: 52.831594634873326\n",
      "-------------------------\n",
      "government\n",
      "Epoch: [1/3], Step: [41/122], Validation Acc: 50.090136492402785\n",
      "Epoch: [1/3], Step: [81/122], Validation Acc: 51.94437290754571\n",
      "Epoch: [1/3], Step: [121/122], Validation Acc: 53.05176409992274\n",
      "Epoch: [2/3], Step: [41/122], Validation Acc: 54.69997424671646\n",
      "Epoch: [2/3], Step: [81/122], Validation Acc: 56.348184393510174\n",
      "Epoch: [2/3], Step: [121/122], Validation Acc: 56.657223796033996\n",
      "Epoch: [3/3], Step: [41/122], Validation Acc: 58.382693793458664\n",
      "Epoch: [3/3], Step: [81/122], Validation Acc: 57.661601854236416\n",
      "Epoch: [3/3], Step: [121/122], Validation Acc: 59.82487767190317\n",
      "-------------------------\n",
      "travel\n",
      "Epoch: [1/3], Step: [41/125], Validation Acc: 50.062735257214555\n",
      "Epoch: [1/3], Step: [81/125], Validation Acc: 53.099121706398996\n",
      "Epoch: [1/3], Step: [121/125], Validation Acc: 53.72647427854454\n",
      "Epoch: [2/3], Step: [41/125], Validation Acc: 54.05269761606023\n",
      "Epoch: [2/3], Step: [81/125], Validation Acc: 55.708908406524465\n",
      "Epoch: [2/3], Step: [121/125], Validation Acc: 55.50815558343789\n",
      "Epoch: [3/3], Step: [41/125], Validation Acc: 58.06775407779172\n",
      "Epoch: [3/3], Step: [81/125], Validation Acc: 59.59849435382685\n",
      "Epoch: [3/3], Step: [121/125], Validation Acc: 60.82810539523212\n",
      "-------------------------\n",
      "telephone\n",
      "Epoch: [1/3], Step: [41/134], Validation Acc: 47.002341920374704\n",
      "Epoch: [1/3], Step: [81/134], Validation Acc: 47.30679156908665\n",
      "Epoch: [1/3], Step: [121/134], Validation Acc: 49.27400468384075\n",
      "Epoch: [2/3], Step: [41/134], Validation Acc: 48.89929742388759\n",
      "Epoch: [2/3], Step: [81/134], Validation Acc: 51.38173302107728\n",
      "Epoch: [2/3], Step: [121/134], Validation Acc: 51.59250585480094\n",
      "Epoch: [3/3], Step: [41/134], Validation Acc: 53.021077283372364\n",
      "Epoch: [3/3], Step: [81/134], Validation Acc: 53.887587822014055\n",
      "Epoch: [3/3], Step: [121/134], Validation Acc: 53.60655737704918\n",
      "-------------------------\n",
      "fiction\n",
      "Epoch: [1/3], Step: [41/120], Validation Acc: 49.76538060479666\n",
      "Epoch: [1/3], Step: [81/120], Validation Acc: 50.1303441084463\n",
      "Epoch: [2/3], Step: [41/120], Validation Acc: 54.066736183524505\n",
      "Epoch: [2/3], Step: [81/120], Validation Acc: 54.197080291970806\n",
      "Epoch: [3/3], Step: [41/120], Validation Acc: 56.777893639207505\n",
      "Epoch: [3/3], Step: [81/120], Validation Acc: 58.57664233576642\n"
     ]
    }
   ],
   "source": [
    "for g in set(val_genre):\n",
    "    print('-------------------------')\n",
    "    print(g)\n",
    "    model = RNN(weights_matrix=loaded_embeddings_ft, hidden_size=100, num_layers=1, num_classes=3) \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.load_state_dict(torch.load('rnn_p_0.pth')['model'])\n",
    "    num_epochs = 3\n",
    "\n",
    "    g_train_loader = getgenreloader_train(g)\n",
    "    g_val_loader = getgenreloader_train(g)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sample in enumerate(g_train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "            output = model(sample[0], sample[1])\n",
    "            label = sample[2]\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "                # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                # validate every 10 iterations\n",
    "            if i > 0 and i % 40 == 0:\n",
    "        #             validate\n",
    "                val_acc = test_model(model, g_val_loader)\n",
    "\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(g_train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
